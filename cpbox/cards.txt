interview problem solving process
    understand the problem aloud, brute force a solution, ask what to optimize (and optimize for it), code, test
### Understand the Problem
        repeat problem to interview
        ask questions to test assumptions
     You should also ask whether you’ll eventually be optimizing for time, space, or both. Even if you believe you already know how to solve the problem, it’s important to ensure that you’re spending your time implementing the solution that your interviewer cares about.
    Keep track of how much time you’re spending in the ideation phase to leave yourself enough time to code your solution.
questions to understand the problem
      - How large is the range of values? What kind of values exists?
      - How large is the size of the input? Are there duplicates within the input?
      - What are some extreme cases of inputs?
      - How is the input stored?
### Brute Force Solution
    makes sure at least some code is written
    You might think that if you present an initial suboptimal solution that you’ll never get hired, but this isn’t true. explain to the interviewer why its suboptimal
    ask the interviewer whether they’d like you to implement the brute force solution or come up with a more efficient solution. Only start coding once you and your interviewer have agreed on an approach and they’ve given you the green light to move ahead.
### Optimize Your Solution
    ask again whether you optimize for time, space, or both. When optimizing, some of your initial assumptions may change, so it’s important to check with your interviewer
    Don’t forget to communicate and analyze the time or space complexity of the solution you plan to optimize. Once you and your interviewer agree that you have a good solution, it’s time to code it.
if you get stuck optimizing
  - Take a second look at the problem, its constraints, and write down your assumptions and possible simplifications.
  - Consider similar problems you’ve seen and discuss them with your interviewer.
  - Solve an example by hand and extrapolate about what algorithm(s) could be used.
  - Simulate the brute-force solution by hand and identify the inefficiencies that could be optimized with a data structure.
  - Solve a simple version of the problem first, then build on it.
### Code Your Solution
    * As an interviewee, it’s important to ensure that your interviewer understands what you’re doing and why you’re doing it in real-time. If you’re explaining your thought process out loud and you take a wrong turn, your interviewer may offer you a hint to get you back on track.
    * While you’re coding, don’t read your code verbatim; but rather, talk about the section of the code you’re implementing at a higher level. Explain why you wrote it that way and what it’s trying to achieve.
    * Remember that your interviewer needs to be able to understand your code to efficiently evaluate whether your code returns the expected result and solves the problem. As you code, keep these pointers in mind:
      - Use good style when writing your code.
      - Naming Matters.
      - Demonstrate good modularity.
      - Leave time to check for bugs and edge cases.
      - If cutting corners, explain it to your interviewer.
    * Take a step back and briefly read through your solution, examining it as if it were written by someone else, but don’t start debugging it manually. This is the same approach that will be taken by your interviewer. Fix any issues you may find.
    * Then walk your interviewer through the code and explain your rationale. If you find a bug or mistake, feel free to acknowledge it and make the change.
### Test Your Solution
    * During the testing portion of the interview, you should have a few goals in mind. You want to…
      - Demonstrate that you care that your code actually works.
      - Show that you can understand and solve problems even if it doesn’t work.
      - Exhibit your understanding of the various edge cases that a real-world problem might present.
      - Establish a feedback loop for your code that allows refactoring without having to worry about breaking existing functionality.
    * There are a few types of inputs you’re going to test for:
      - Large and small valid inputs.
      - Large and small invalid inputs.
      - Random input.
    * For each input, manually go through your code and ensure that it performs. You should jot down or tell your interviewer the values of each variable as you walk them through the code.
    * If you find an error, explain the error to your interviewer, fix it, and move on. Similarly, if there are large duplicated code blocks in your solution, you can take the time to restructure the code.
    * In this stage, you’ll also have the opportunity to showcase your knowledge around automated testing. You can chat about how you’d generate different types of inputs, how you would stub parts of the code, the test runners you’d use, and more. These are all great signals to show an interviewer.
Bubble Sort
    - This simple sorting algorithm iterates over a list, comparing elements in pairs and swapping them until the larger elements "bubble up" to the end of the list, and the smaller elements stay at the "bottom".
Bubble Sort time complexity
    - In the worst case scenario (when the list is in reverse order), this algorithm would have to swap every single item of the array. Therefore, if we have n elements in our list, we would have n iterations per item - thus Bubble Sort's time complexity is O(n^2).
    O(n^2)
#### Selection Sort
    - This algorithm segments the list into two parts: sorted and unsorted. We continuously remove the smallest element of the unsorted segment of the list and append it to the sorted segment.
#### Selection Sort time complexity
    - For a list with n elements, the outer loop iterates n times. The inner loop iterate n-1 when i is equal to 1, and then n-2 as i is equal to 2 and so forth. The amount of comparisons are `(n - 1) + (n - 2) + ... + 1`, which gives Selection Sort a time complexity of O(n^2).
#### Insertion Sort
    - Like Selection Sort, this algorithm segments the list into sorted and unsorted parts. It iterates over the unsorted segment, and inserts the element being viewed into the correct position of the sorted list.
#### Insertion Sort time complexity
    - In the worst case scenario, an array would be sorted in reverse order. The outer for loop in Insertion Sort function always iterates n-1 times. In the worst case scenario, the inner for loop would swap once, then swap two and so forth. The amount of swaps would then be `1 + 2 + ... + (n - 3) + (n - 2) + (n - 1)` which gives Insertion Sort a time complexity of O(n^2).
#### Heap Sort
    - This popular sorting algorithm, like the Insertion and Selection sorts, segments the list into sorted and unsorted parts. It converts the unsorted segment of the list to a Heap data structure, so that we can efficiently determine the largest element.
#### Heap Sort time complexity
    - Let's first look at the time complexity of the `heapify` function. In the worst case the largest element is never the root element, this causes a recursive call to `heapify`. While recursive calls might seem dauntingly expensive, remember that we're working with a binary tree. Visualize a binary tree with 3 elements, it has a height of 2. Now visualize a binary tree with 7 elements, it has a height of 3. The tree grows logarithmically to n. The `heapify` function traverses that tree in O(log(n)) time.
    - The `heap_sort` function iterates over the array n times. Therefore the overall time complexity of the Heap Sort algorithm is O(nlog(n)).
#### Merge Sort
    - This divide and conquer algorithm splits a list in half, and keeps splitting the list by 2 until it only has singular elements. Adjacent elements become sorted pairs, then sorted pairs are merged and sorted with other pairs as well. This process continues until we get a sorted list with all the elements of the unsorted input list.
#### Merge Sort time complexity
    - Let's first look at the `merge` function. It takes two lists, and iterates n times, where n is the size of their combined input.
    - The `merge_sort` function splits its given array in 2, and recursively sorts the sub-arrays. As the input being recursed is half of what was given, like binary trees this makes the time it takes to process grow logarithmically to n. Therefore the overall time complexity of the Merge Sort algorithm is O(nlog(n)).
#### Quick Sort
    - This divide and conquer algorithm is the most often used sorting algorithm. When configured correctly, it's extremely efficient and does not require the extra space Merge Sort uses. We partition the list around a pivot element, sorting values around the pivot.
#### Quick Sort time complexity
    - The worst case scenario is when the smallest or largest element is always selected as the pivot. This would create partitions of size n-1, causing recursive calls n-1 times. This leads us to a worst case time complexity of O(n^2).
    - While this is a terrible worst case, Quick Sort is heavily used because it's average time complexity is much quicker. While the `partition` function utilizes nested while loops, it does comparisons on all elements of the array to make its swaps. As such, it has a time complexity of O(n).
    - With a good pivot, the Quick Sort function would partition the array into halves which grows logarithmically with n. Therefore the average time complexity of the Quick Sort algorithm is O(nlog(n)).
![Sorting-Comparisons](assets/Sorting-Comparison.png)
* Depth-First traversal
    We completely traverse one sub-tree before exploring a sibling sub-tree (pre-oder, in-order, post-order). uses stack
    ```
    InOrderTraversal(tree)
    
    if tree = nil: return
    InOrderTraversal(tree.left)
    Print(tree.key)
    InOrderTraversal(tree.right)
    ```
* Breadth-First:  traversal
    We traverse all nodes at one level before progressing to the next level. uses queue
    **Breadth-First**
    
    ```
    LevelTraversal(tree)
    
    if tree = nil: return
    Queue q
    q.Enqueue(tree)
    while not q.Empty():
      node <- q.Dequeue()
      Print(node)
      if node.left != nil:
        q.Enqueue(node.left)
      if node.right != nil:
        q.Enqueue(node.right)
    ```
### Graph root
    - Graph traversal can begin with any node, since there is no concept of a root node.
graph bfs use
    - A unique thing about BFS is that it lends itself quite nicely to determining the **shortest path** between any node in the graph and the “parent” node.
      - In fact, most BFS implementations will keep track of every single node’s “parent” nodes that come before it.
      - This is helpful because we can use the pointers of the path that we take to get to a node from the starting node in order to determine a shortest path in the graph.
    But if the graph is wide, we'd need to store all of it, level by level, with references and using memory unnecessarily!
graph bfs time complexity
    - If we must visit every node once, and check every edge in its adjacency list, the runtime complexity for both a directed and undirected graph is the sum of the vertices and their edges as represented by the graph in its adjacency list representation, or **O(V + E)**.
    
graph DFS Traversals** use
      - DFS tells us if a path even exists.
    - DFS is like solving a maze:
      - We'll continue to walk through the path of the maze until we reach a dead end.
      - When we do reach a dead end, we backtrack until we find another path we haven't walked yet, and repeat.
      - Eventually, we will be able to determine if we can get out of the maze.
  - Not helpful in finding shortest paths. We could end up following the longest path from node x to y!
  - But if the graph is deep enough, it can be great since we don't need to store the entire thing in memory!
dfs time complexity
    - DFS requires **O(V + E)** runtime:
      - For a directed graph, |E| edges to check.
      - For an undirected graph, 2|E| edges (each edge is visited twice).
### Backtracking
    * Backtracking solves problems by trying a series of actions. If a series fails, we back up and try a different series.
    * A maze is a classic example, but the approach can be used on a wide variety of problems.
    * The recursive solution may be elegant, but remember not to dwell on the backtracking concept or what the recursion is doing.
### Binary Search
    Binary search is an efficient algorithm for finding an item from a sorted list of items. It works by repeatedly dividing in half the portion of the list that could contain the item, until you've narrowed down the possible locations to just one.
    
    We basically ignore half of the elements just after one comparison.
    1. Compare x with the middle element.
    2. If x matches with middle element, we return the mid index.
    3. Else If x is greater than the mid element, then x can only lie in right half subarray after the mid element. So we recur for right half.
    4. Else (x is smaller) recur for the left half.

### Breadth First Search
    1. Pick any node, visit the adjacent unvisited vertex, mark it as visited, display it, and insert it in a queue.
    2. If there are no remaining adjacent vertices left, remove the first vertex from the queue.
    3. Repeat step 1 and step 2 until the queue is empty or the desired node is found.
    - Since all of ​the nodes and vertices are visited, the time complexity for BFS on a graph is O(V + E); where V is the number of vertices and E is the number of edges.
### Depth First Search
    Depth First Search (DFS) is a widely used interview question. Learn about it in [this video](https://www.coursera.org/lecture/algorithms-part2/depth-first-search-mW9aG). It is an algorithm for tree traversal on graph or tree data structures. It can be implemented easily using recursion and data structures like dictionaries and arrays.
    **The Algorithm**
    1. Pick any node. If it is unvisited, mark it as visited and recur on all its adjacent nodes.
    2. Repeat until all the nodes are visited, or the node to be searched is found.
    
    - Since all of ​the nodes and vertices are visited, the time complexity for DFS on a graph is O(V + E); where V is the number of vertices and E is the number of edges. In case of DFS on a tree, the time complexity is O(V), where V is the number of nodes.
### Recursion
    * Used when problems exhibit common sub-structures.
### Trees
    - A tree is a hierarchical data structure consisting of vertices (nodes) and edges that connect them. Trees are similar to graphs, but the key point that differentiates a tree from the graph is that a cycle cannot exist in a tree.
    - Trees are extensively used in Artificial Intelligence and complex algorithms to provide an efficient storage mechanism for problem-solving.
    - Trees are *recursive data structures* because a tree is usually composed of smaller trees — often referred to as subtrees — inside of it. The child of one node in a tree could very well be the parent of another tree (making it the root node of a subtree). This can be really interesting when it comes to writing algorithms to traverse or search through a tree, since the nesting of trees can sometimes lead us to writing recursive search algorithms.
balanced tree
      - A tree is considered to be **balanced** if any two sibling subtrees do not differ in height by more than one level.
      - However, if two sibling subtrees differ significantly in height (and have more than one level of depth of difference), the tree is **unbalanced**.
number of edges in a tree
    - *If a tree has n nodes, it will always have one less number of edges (n-1).*
The following are the types of trees:
    * N-ary Tree
    * Balanced Tree
    * Binary Tree
    * Binary Search Tree
    * AVL Tree
    * Red Black Tree
    * 2–3 Tree
    Out of the above, Binary Tree and Binary Search Tree are the most commonly used trees.

A **binary tree** is a non linear data structure where each node can have at most 2 child nodes.
    - The worst case complexity for searching a node is O(n).

**Binary search tree** is a binary tree in which a node have a value greater than all values in its left subtree and smaller than all values in its right subtree.
    - If a Binary search tree is balanced then you can find a value in O(log n) time.
- An **edge list** 
    is a list/array of all of the |E| edges in a graph. Edge lists are one of the simplest representations of a graph.
edge list time complexity
  - It would take **O(E)** to find one particular edge.
edge list space complexity
  - The space of representing an edge list is also going to be **O(E)**!

- An **adjacency matrix** 
    is a matrix that represents exactly which vertices/nodes in a graph have edges between them. The matrix serves as a lookup table, where a value of **1** represents an edge that exists, and **0** represents an edge that does not.
  - They are easy to follow and represent.
adjacency matrix Looking up, inserting, and removing an edge time complexity
    can be done in **O(1)** (**constant** time).
adjacency matrix space complexity
  - However, they can take up more space than necessary. An adjacency matrix always consumes **O(V^2)** amount of space.
  - If a graph is **dense** and has many edges, this isn't too bad. But if a graph has few edges and is **sparse**, the matrix will be mostly filled with **0**'s, but take up lots of space!

- An **adjacency list** 
    is an array of linked lists that servers the purpose of representing a graph, but also makes it easy to see which other vertices are adjacent to other vertices. Each vertex in a graph can easily reference its neighbors through a **linked list**.
adjacency list time complexity to retrieve a vertex's possible neighbors
  - Retrieving one vertex's possible neighbors takes **O(1)** time, sine we only need the vertex's index to get its list of adjacent neighbors.
adjacency list time complexity to retrieve a specific edge
  - To find a specific edge **(x, y)**, we need to find vertex **x**'s adjacency list, which takes constant time, and look to see if **y** is in that list.
  - In the worst case, this takes **O(d)** time, where **d** is the degree of vertex **v**.
adjacency list space complexity 
  - An adjacency list will require **|V|** amount of space for the list itself, as every vertex must be represented in the list.

A graph can be easily presented using the python dictionary data types. We represent the vertices as the keys of the dictionary and the connection between the vertices also called edges as the values in the dictionary. Here are basic operations:
    * To **display graph vertices**, we simple find the keys of the graph dictionary. We use the keys() method.
    * To **find graph edges**, we have to find each of the pairs of vertices which have an edge in between them. So we create an empty list of edges then iterate through the edge values associated with each of the vertices. A list is formed containing the distinct group of edges found from the vertices.
    * To **add a vertex**, we add another additional key to the graph dictionary.
    * To **add an edge** to an existing graph, we treat the new vertex as a tuple and validating if the edge is already present. If not then the edge is added.
### Hash Tables

    Hashing is a process used to uniquely identify objects and store each object at some pre-calculated unique index called its “key.” So, the object is stored in the form of a “key-value” pair, and the collection of such items is called a “dictionary.” Each object can be searched using that key. There are different data structures based on hashing, but the most commonly used data structure is the **hash table**.
    
    Hash tables are made up of two distinct parts: **an array** (which is the actual table where the data will be stored) and **a hash function** (which is responsible for taking input data and creating a mapping by assigning it a very specific index in the array). Its search, insert, and delete operations all take O(1) average complexity and O(n) worst complexity.
    
What makes a good hash table?
    - **It should be easy to compute**. Hard-to-compute hash functions mean that we lose any gained advantages for quick and efficient lookup time!
    - **It should avoid collision**. Collisions are unavoidable but the more collisions, the harder it is to come up with a fast, efficient algorithm for resolving them!
    - **It should use all the input data, and always return the same key for the same hash bucket per value**. If a hash function doesn't always return the same value for any given input, we'll never be able to reliably access data from it!

The performance of hashing data structure depends upon these three factors:
    - **Hash Function**: A function that maps a big number or string to a small integer that can be used as the index in the hash table.
    - **Size of the Hash Table**: Changing the size of the hash table will change the compression function, leading to the keys being allotted to different buckets.
    - **Collision Handling Method**: Hash function may return the same hash value for 2 or more keys (collision). We can resolve this problem either via separate chaining or open addressing.

The two most common collisions resolution tactics are:
    - **Linear Probing**:
      - If a hash function encounters a collision, it can resolve it by finding the next available hash bucket for the data, and effectively probing through the table until it finds an empty space.
      - The downside to linear probing is its tendency for clustering. A poorly-designed hash table will "cluster" a majority of its input data into only a few hash buckets. If the hash bucket at the next key is also not available, the hash function loops back through the table.
    - **Chaining**:
      - If a hash function encounters a collision, it can resolve it by using a linked list to store a collection of data at one hash bucket key, meaning that it can simply chain on more data to the same slot in the hash table.
      - The downside to chaining is that it takes more time to search with more items at one location - O(n) - where n is the number of items in the hash bucket.
      - With a good hash function, chaining still averages out to have a search time of O(1).
    
### Set
    A **set** is nothing more than an unordered collection of elements with absolutely no duplicates.
Here are basic operations:
    - **Intersection**: *X and Y* yields another set of all the elements that are both in X and Y.
    - **Union**: *X or Y* yields another set of all the elements that are either in X or in Y.
    - **Set Differences**: *X - Y* yields the difference between two sets, or all the elements in set X that are not in set Y.
    - **Relative Complement**: *Y \ X* yields a set with all of the elements in set Y that do not also exist in set X.
    - **Symmetric Difference / Disjunctive Union**: *X delta Y* yields all the elements that exist in either of the sets, but do not exist in the intersection *X and Y* of the two sets.

The time complexity of set operations:
    - *O(length(X) + length(Y))* for intersection, union, difference/complement: This is because in order for us to find the intersection, union, or difference/complement of these two sets, we have to effectively traverse through the entire length of the two sets being compared.
    - *O(1)* for add, remove, search, get length: This can be incredibly powerful, and often means that a set might be a better structure than a dictionary or a hash!
    
Hash tables are often used to implement sets!
    - First, given what we know about hash tables, they’ll always have unique keys for each element. This is great for sets, since sets can only have unique values in them.
    - Second, in hash tables, order doesn’t really matter, just as how order doesn’t matter in a set.
    - Finally, hash tables provide a O(1) constant access time, which is what ideal for basic operations performed on a set.

### Heaps
    A **heap** is really nothing more than a binary tree with some additional rules that it has to follow. These two rules are the two properties that define what differentiates a heap structure from any other tree structure.
    - **Shape**: A heap must be a **complete** binary tree. This means that all of the levels of the tree must be completely filled - except may be the last one. Also, the last level must have the left-most nodes filled, always!
    - **Order**: A heap's root node must have all of its children be either greater than or equal to its children (**min-heap**), or less than or equal to its children (**max-heap**).
    
**Min-Heap vs Max-Heap**
    - These two formats are the two ways that a heap can be ordered - also called the **heap-order property**.
    - In a **min-heap**, every single parent node (including the root) is less than or equal to the value of its children nodes. The *minimum* value key is always at the root node.
    - In a **max-heap**, every single parent node (including the root) is greater than or equal to the value of its children nodes. The *maximum* value key is always at the root node.
- We can always have duplicate values in a heap — there’s no restriction against that.
- Unlike binary search trees, the left node does not have to be smaller than the right node!

**Growking and Shrinking A Heap**
    - We must always maintain the shape and structure of a heap — otherwise, we’re violating one of its two properties!
    - When **growing** a heap, we can only ever add a node to the left-most available spot in the tree; that is to say, the left most available node, at the lowest possible level. Then we can swap nodes that are out of order.
    - Most heaps **remove** the root node. The only node we can remove and replace the root node with is the left-most, lowest-level leaf node. Then, we continue to "bubble down" the current root node until we are no longer violating the heap-order property - that is, until all the parent nodes are greater than or equal to their child nodes.

**Heap Implementation**
    - If we know the index of the root node, we can manipulate that index in order to determine where its child nodes would be located within that same array representation of the heap.
    - If i = index of a parent node, then 2i + 1 = index of right child node and 2i + 2 = index of left child node.
    - If n = index of the current node, then (n-1)/2 = index of current node's parent.

**Heap and Priority Queue**
    - A **priority queue** is a queue data structure with some additional properties:
      - Every item has a priority associated with it - usually an integer.
      - An item with a high priority is de-queued before an item with a low priority.
      - Two items with an equal priority are de-queued based on their order in the queue.
    - Binary heaps are efficient ways of implementing priority queue.
     - Finding the max/min element takes constant time (O(1)).
     - Insertion/deletion takes logarithmic time (O(logn)).

### Tries
    A **trie** is a tree-like data structure whose nodes store the letters of an alphabet. By structuring the nodes in a particular way, words and strings can be retrieved from the structure by traversing down a branch path of the tree.
    - The shape and structure of a trie is always a set of linked nodes, all connecting back to an empty root node. Each node contains an array of pointers (child "nodes"), one for each possible alphabetic value.
    - The size of a trie is directly connected/correlated to the size of the alphabet that is being represented.

**What’s in a single node of a trie?**
    - A single node in a trie contains just two things:
      - A value, which might be null.
      - An array of references to child nodes, all of which also might be null.
    - When a trie representing the English language is created, it consists of a single root node, whose value is usually set to an empty string: "".
      - That root node will also have an array that contains 26 references, all of which will point to null at first.
      - As the trie grows, those pointers start to get filled up with references to other nodes nodes.

**Trie Operations**
    - Search through a trie:
      - If we search for the word/key "pie" and are able to find it, we can look to see if it has a value, and return it.
      - If we search for the key "pi" and find the node at the last letter "i", we can look to see its value. If it has a NULL value, it means that "pi" is not currently a key.
    - Delete from a trie:
      - If we want to remove a key/value pair, we first find the node for that key, and set its value to NULL.
      - Once the node has a NULL value, check if its references are NULL. If they are, we can remove it, and repeat/work our way back up the trie.
    
**Tries vs Hash Tables**
    - Hash tables use arrays combined with linked lists, whereas tries use arrays combined with pointers.
    - A trie doesn't need a **hash function**, because every path to an element will be unique.
    - However, a trie takes up a lot of memory/space with empty/NULL pointers.
    
**How Tries Changes As They Grow**
    - As a trie grows in single, less work must be done to add a value, since the "intermediate nodes", or the branches of the trie have already been built up.
    - Each time we add a word's letter, we need to look at 26 references, since there are only 26 possibilities/letters in the alphabet. This number never changes in the context of our trie, so it is a **constant value**.
    
**Big O Notation of A Trie Structure**
    - The worst case runtime for **creating** a trie structure depends on how many words the trie contains, and how long they might potentially be. This is known as **O(mn)** time, where **m** is the longest word and **n** is the total number of words.
    - The time of **searching**, **inserting**, and **deleting** from a trie depends on the length of the word **a** and the total number of words: **O(an)**.
    
### Understanding race conditions
    Race conditions affect the correctness of concurrent programs. Learn about race conditions and the related concept of interleaving in [this video](https://www.coursera.org/lecture/golang-concurrency/m2-2-1-2v3-dybTl). You can also read about race conditions in [this MIT page](https://web.mit.edu/6.005/www/fa15/classes/19-concurrency/#race_condition).
    - A **race condition** means that the correctness of the program (the satisfaction of postconditions and invariants) depends on the relative timing of events in concurrent computations A and B. When this happens, we say “A is in a race with B.”
    - Some inter-leavings of events may be OK, in the sense that they are consistent with what a single, non-concurrent process would produce, but other inter-leavings produce wrong answers – violating postconditions or invariants.
### Understanding the difference between concurrency and parallelism
    Concurrency and parallelism are fundamental concepts in software engineering. Learn about the differences between these two concepts in [this video](https://www.coursera.org/lecture/parprog1/introduction-to-parallel-computing-zNrIS) (minute 6:32). You can read about concurrency in [this Wikipedia article](https://en.wikipedia.org/wiki/Concurrent_computing) and [this MIT page](https://web.mit.edu/6.005/www/fa14/classes/17-concurrency/#reading_17_concurrency). Also, you can read about parallelism in [this Wikipedia article](https://en.wikipedia.org/wiki/Parallel_computing).
    - **Parallelism** is a type of computation in which many calculations are performed at the same time.
      - Basic principle: computation can be divided into smaller subproblems, each of which can be solved simultaneously.
      - Assumption: we have parallel hardware at our disposal, which is capable of executing these computations in parallel.
- Why Parallelism?
      - Parallel programming is much harder than sequential programming. Separating sequential computations into parallel subcomputations can be challenging, or even impossible.
      - Ensuring program correctness is more difficult, due to new types of errors.
      - *Speedup* is the only reason why we bother paying for this complexity.

- **Concurrency** means multiple computations are happening at the same time. Concurrency is everywhere in modern programming, whether we like it or not:
      - (1) Multiple computers in a network,
      - (2) Multiple applications running on one computer,
      - (3) Multiple processors in a computer (today, often multiple processor cores on a single chip)
    - In fact, concurrency is essential in modern programming:
      - Web sites must handle multiple simultaneous users.
      - Mobile apps need to do some of their processing on servers (“in the cloud”).
      - Graphical user interfaces almost always require background work that does not interrupt the user. For example, Eclipse compiles your Java code while you’re still editing it.
    
Parallelism and concurrency are closely related concepts:
    - Parallel program uses parallel hardware to execute computation more quickly. Efficiency is its main concern.
    - Concurrent program may or may not execute multiple executions at the same time. Improves modularity, responsiveness or maintainability.

### Analyzing large files
    - Count with `wc`.
    - Concatenate files with `cat`.
    - Modify a file with `sed`.
    - Subset a large file with `head` and `tail`.
    - Finding duplicates with `uniq`.
    - Selecting columns with `cut`.
    - Looping with `while true; do ... done` to process or transfer large number of files.
    - Create variables with `varname`.

### Differentiate the stack from the heap
    The stack is the memory set aside as scratch space for a thread of execution. The heap is memory set aside for dynamic allocation. Read [this thread](https://stackoverflow.com/questions/79923/what-and-where-are-the-stack-and-heap#comment67326550_80113) for more information.
    - The **stack** is the memory set aside as scratch space for a thread of execution.
      - When a function is called, a block is reserved on the top of the stack for local variables and some bookkeeping data. When that function returns, the block becomes unused and can be used the next time a function is called.
      - The stack is always reserved in a LIFO (last in first out) order; the most recently reserved block is always the next block to be freed. This makes it really simple to keep track of the stack; freeing a block from the stack is nothing more than adjusting one pointer.
    - The **heap** is memory set aside for dynamic allocation.
      - Unlike the stack, there's no enforced pattern to the allocation and deallocation of blocks from the heap; you can allocate a block at any time and free it at any time.
      - This makes it much more complex to keep track of which parts of the heap are allocated or free at any given time; there are many custom heap allocators available to tune heap performance for different usage patterns.
    - Each thread gets a stack, while there's typically only one heap for the application (although it isn't uncommon to have multiple heaps for different types of allocation).
    
    - *To what extent are they controlled by the OS or language runtime?*
      - The OS allocates the stack for each system-level thread when the thread is created.
      - Typically the OS is called by the language runtime to allocate the heap for the application.
    - *What is their scope?*
      - The stack is attached to a thread, so when the thread exits the stack is reclaimed.
      - The heap is typically allocated at application startup by the runtime, and is reclaimed when the application (technically process) exits.
    - *What determines the size of each of them?*
      - The size of the stack is set when a thread is created.
      - The size of the heap is set on application startup, but can grow as space is needed (the allocator requests more memory from the operating system).
    - *What makes one faster?*
      - The stack is faster because the access pattern makes it trivial to allocate and deallocate memory from it (a pointer/integer is simply incremented or decremented), while the heap has much more complex bookkeeping involved in an allocation or deallocation.
      - Also, each byte in the stack tends to be reused very frequently which means it tends to be mapped to the processor's cache, making it very fast.
      - Another performance hit for the heap is that the heap, being mostly a global resource, typically has to be multi-threading safe, i.e. each allocation and deallocation needs to be - typically - synchronized with "all" other heap accesses in the program.
    
### Differentiating Monolithic vs Microservices architectures
    |                      |                                       Monolithic                                      |                                 Microservice                                 |
    |:--------------------:|:-------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------:|
    |         Size         |                               Single self-contained unit                              |               Very small function-oriented independent services              |
    |      Granularity     |                           Tightly coupled with low cohesion                           |                      Loosely coupled with high cohesion                      |
    |  Ease of Deployment  |                 Requires recreating and redeploying entire application                |             Each service can be built and deployed independently             |
    | Remote Call Overhead |                                        Low/None                                       |          High communication overhead due to increase in remote calls         |
    |  Speed of Deployment |                              Very slow deployment speeds                              |                        Rapid and continuous deployment                       |
    |      Persistence     |              All services in a monolithic application share data storage              |              Each service is free to choose its own data storage             |
    |  Ease of On-Boarding |                      Can be difficult to on-board new developers                      |                        Easy to on-board new developers                       |
    | Polyglot Programming |                           Utilize a single technology stack                           |             Can utilize a different technology stack per service             |
    | Communication Method |                           Language-level or procedure calls                           |        Communicates via API layer with lightweight protocols like REST       |
    |      Scalability     | Horizontally scalable, can be very challenging to scale as application becomes larger | Vertically and horizontally scalable through use of containers and the cloud |

### Understanding the role of the Hostname in a request
    - In the Internet, a hostname is a domain name assigned to a host computer. This is usually a combination of the host's local name with its parent domain's name. For example, `en.wikipedia.org` consists of a local hostname (`en`) and the domain name `wikipedia.org`. This kind of hostname is translated into an IP address via the local hosts file, or the Domain Name System (DNS) resolver. It is possible for a single host computer to have several hostnames; but generally the operating system of the host prefers to have one hostname that the host uses for itself.
    - Any domain name can also be a hostname, as long as the restrictions mentioned below are followed. So, for example, both `en.wikipedia.org` and `wikipedia.org` are hostnames because they both have IP addresses assigned to them. A hostname may be a domain name, if it is properly organized into the domain name system. A domain name may be a hostname if it has been assigned to an Internet host and associated with the host's IP address.
    
**What are classes?**
    
    ```
    # This is the template for making PartyAnimal objects, where "class" is a reserved word
    class PartyAnimal:
      x = 0 # Each PartyAnimal object has a bit of data
    
      # Each PartyAnimal object has a bit of code
      def party(self):
        self.x = self.x + 1
        print("So far", self.x)
    
    # Construct a PartyAnimal object and store in "an" variable
    an = PartyAnimal()
    
    # Tell the "an" object to run the party() code within it (equivalent to PartyAnimal.party(an))
    an.party()
    ```

**What is inheritance?**
    - A way to structure our code so multiple classes can share common fields, properties, and methods.
    - Classes can still have class-specific data and operations as well.
http `GET` method 
    requests a representation of the specified resource. Requests using `GET` should only retrieve data.
http `HEAD` method 
    asks for a response identical to that of a `GET` request, but without the response body.
http `POST` method 
    is used to submit an entity to the specified resource, often causing a change in state or side effects on the server.
http `PUT` method 
    replaces all current representations of the target resource with the request payload.
http `DELETE` method 
    deletes the specified resource.
http `CONNECT` method 
    establishes a tunnel to the server identified by the target resource.
http `OPTIONS` method 
    is used to describe the communication options for the target resource.
http `TRACE` method 
    performs a message loop-back test along the path to the target resource.
http `PATCH` method 
    is used to apply partial modifications to a resource.
### Understanding the single responsibility principle
    - The Single Responsibility Principle (SRP) states that each software module should have one and only one reason to change.
    - When you write a software module, you want to make sure that when changes are requested, those changes can only originate from a single person, or rather, a single tightly coupled group of people representing a single narrowly defined business function. You want to isolate your modules from the complexities of the organization as a whole, and design your systems such that each module is responsible (responds to) the needs of just that one business function.
    - Another wording for the Single Responsibility Principle is: **Gather together the things that change for the same reasons. Separate those things that change for different reasons.**
    - If you think about this you’ll realize that this is just another way to define cohesion and coupling. We want to increase the cohesion between things that change for the same reasons, and we want to decrease the coupling between those things that change for different reasons.
### Understanding the benefits of continuous integration
    - In continuous integration:
      - Work in small batches and build quality in.
      - Keep branches short-lived and integrate them into trunk frequently.
      - The first priority is to solve the problem.
    - Closely related concepts are **continuous testing** and **continous delivery**:
      - In CT: Testing is an ongoing development process. Engineers run automated unit and acceptance tests against every commit to version control. This gives them fast feedback on their changes.
      - Implementing CD means creating multiple feedback loops to ensure that high-quality software gets delivered to users more frequently and more reliably.
### What Are Design Fundamentals?
    - Categorized into 4 parts: Foundational Knowledge, Key Characteristics, Components, and Tech Services.
    - From general to specific concepts.
**IP Address**
    - An address given to each machine connected to the public internet. IPv4 addresses consist of 4 numbers separated by dots: **a.b.c.d** where all 4 numbers are between 0 and 255. Special values include:
      - **127.0.0.1**: Your own local machine. Also referred to as **localhost**.
      - **192.168.x.y**: Your private network. For instance, your machine and all machines on your private WIFI network will usually have the **192.168** prefix.
**DNS**
    - Short for Domain Name System, it describes the entities and protocols involved in the translation from domain names to IP Addresses.
    - Typically, machines make a DNS query to a well-known entity which is responsible for returning the IP address (or multiple ones) of the requested domain name in the response.
### Network Protocols
    - A protocol is an agreed upon set of rules for interaction between two parties. In the context of networking, the protocol happens between two machines (client and server): content, format, order of the messages sent between them.

**IP (Internet Protocol)**
    - IP outlines how almost all machine-to-machine communications should happen in the world.
    - Other protocols like **TCP**, **UDP**, and **HTTP** are built on top of IP.

**TCP (Transmission Control Protocol)**
    - TCP is built on top of the IP's data section.
    - It allows for ordered and reliable data delivery between machines over the public internet by creating a **connection**.
    - TCP is usually implemented in the kernel, which exposes **sockets** to applications that they can use to stream data through an open connection.

**HTTP 
    (HyperText Transfer Protocol)**
    - HTTTP is built on top of TCP.

**IP Packet**
    - An IP packet is effectively the smallest unit used to describe data being sent over **IP**, aside from bytes.
    - It consists of:
      - an **IP header**, which contains the source and destination **IP addresses** as well as other information related to the network.
      - a **payload**, which is just the data being sent over the network.

**Databases**
    - Databases are programs that either use disk or memory to do 2 core things: **record** data and **query** data.
    - In general, they are themselves servers that are long-lived and interact with the rest of your application through network calls, with protocols on top of TCP or even HTTP.
    - Some databases only keep records in memory, and the users of such databases are aware of the fact that those records may be lost forever if the machine or process dies.
    - For the most part though, databases need persistence of those records, and thus cannot use memory. This means that you have to write your data to disk. Anything written to disk will remain through power loss or network partitions, so that's what is used to keep permanent records.
    - Since machines die often in a large-scale system, special disk partitions or volumes are used by the database processes, and those volumes can get recovered even if the machine were to go down permanently.
**Persistent Storage**
    - Usually refers to disk, but in general, it is any form of storage that persists if the process in charge of managing it dies.
**Latency**
    - The time it takes for a certain operation to complete in a system.
    - Most often, this measure is a time duration, like milli-seconds or seconds. Below are common orders of magnitude:
latency Reading 1 MB from RAM**: 
    0.25 ms
latency Reading 1 MB from SSD**: 
    1 ms
latency Transfer 1 MB over Network**: 
    10 ms
latency Reading 1 MB from HDD**: 
    20 ms
latency Inter-Continental Round Trip**: 
    150 ms
**Throughput**
    - The number of operations that a system can handle properly per time unit.
    - For instance, the throughput of a server can often be measured in requests per second.
Process assumption
    You should always assume that any process may get terminated at any time in a sufficiently large system.
**Node/Instance/Host**
    - These three terms refer to the same thing most of the time: a virtual or physical machine on which the developer runs processes.
    - Sometimes the word **server** also refers to this same concept.
**Availability**
    - The odds of a particular server or service being up and running at any point in time, usually measured in percentages.
    - A server that has 99% availability will be operational 99% of the time (this would be described as having two **nines** of availability).
**High Availability**
    - Used to describe systems that have particularly high levels of availability, typically 5 nines or more.
**Nines**
    - Typically refers to percentages of uptime.
    - For example, 5 nines of availability means an uptime of 99.999% of the time.
**Redundancy**
    - The process of replicating parts of a system in an effort to make it more reliable.
**SLA**
    - Short for "service-level agreement", an SLA is a collection of guarantees given to a customer by a service provider.
    - SLAs typically make guarantees on a systems's availability, amongst other things.
    - SLAs are made up of one or multiple SLOs.
**SLO**
    - Short for "service-level objective", an SLO is a guarantee given to a customer by a service provider.
    - SLOs typically make guarantees on a systems's availability, amongst other things.
    - SLOs constitute an SLA.
**Cache**
    - A piece of hardware or software that stores data, typically meant to retrieve that data faster than otherwise.
    - Caches are often used to store responses to network requests as well as results of computationally-long operations.
    - Note that data in a cache can become **stale** if the main source of truth for that data (i.e., the main database behind the cache) gets updated and the cache doesn't.
**Cache Hit**
    - When requested data is found in a cache.
**Cache Miss**
    - When requested data could have been found in a cache but isn't. This is typically used to refer to a negative consequence of a system failure or of a poor desgin choice.
    - For example: *If a server goes down, our load balancer will have to forward requests to a new server, which will result in cache misses*.
**Cache Eviction Policy**
    - The policy by which values get evicted or removed from a cache.
    - Popular cache eviction policies include **LRU** (Least Recently Used), **FIFO** (First-In, First-Out), and **LFU** (Least Frequently Used).
**Content Delivery Network**
    - A **CDN** is a 3rd-party service that acts like a cache for your servers.
    - Sometimes, web applications can be slow for users in a particular region if your servers are located only in another region. A CDN has servers all around the world, meaning that the latency to a CDN's servers will almost always be far better than the latency to your servers.
    - A CDN's servers are often referred to as **PoPs** (Points of Presence). Two of the most popular CDNs are **Cloudflare** and **Google Cloud CDN**.
**Forward Proxy**
    - A server that sits between a client and servers and acts on behalf of the client, typically used to mask the client's identity (IP address). Note that forward proxies are often referred to as just proxies.
**Reverse Proxy**
    - A server that sits between clients and servers and acts on behalf of the servers, typically used for logging, load balancing, or caching.
**Nginx**
    - A very popular webserver that's often used as a **reverse proxy** and **load balancer**.
**Load Balancer**
    - A type of **reverse proxy** that distributes traffic across servers.
    - Load balancers can be found in many parts of a system, from the DNS layer all the way to the database layer.
**Server-Selection Strategy**
    - How a **load balancer** chooses servers when distributing traffic amongst multiple servers.
    - Commonly used strategies include round-robin, random selection, performance-based selection, and IP-based routing.
**Hot Spot**
    - When distributing a workload across a set of servers, that workload might be spread unevenly.
    - This can happen if your **sharding key** or your **hashing function** are suboptimal, or if your workload is naturally skewed: some servers will receive a lot more traffic than others, thus creating a "hot spot".
**Consistent Hashing**
    - A type of hashing that minimizes the number of keys that need to be remapped when a hash table gets resized.
    - It's often used by load balancers to distribute traffic to servers.
    - It minimizes the number of requests that get forwarded to different servers when new servers are added or when existing servers are brought down.
**Rendezvous Hashing**
    - A type of hashing also coined highest random weight hashing.
    - Allows for minimal re-distribution of mappings when a server goes down.
**SHA**
    - Short for "Secure Hash Algorithms", the SHA is a collection of cryptographic hash functions used in the industry.
    - These days, SHA-3 is a popular choice to use in a system.
**Relational Database**
    - A type of structured database in which data is stored following a tabular format; often supports powerful querying using SQL.
**Non-Relational Database**
    - In contrast with relational database, a type of database that is free of imposed, tabular-like structure.
    - Non-relational databases are often referred to as NoSQL databases.
sql db
    - This term is often used synonymously with "Relational Database," though in practice, not every relational database supports SQL.
**NoSQL Database**
    - Any database that is not SQL compatible is called NoSQL.
**ACID Transaction**
    A type of database transaction that has 4 important properties:
    - **Atomicity** - The operations that constitute the transaction will either all succeed or all fail. There is no in-between state.
    - **Consistency** - The transaction cannot bring the database to an invalid state. After the transaction is committed or rolled back, the rules for each record will still apply, and all future transactions will see the effect of the transaction.
    - **Isolation** - The execution of multiple transactions concurrently will have the same effect as if they had been executed sequentially.
    - **Durability** - Any committed transaction is written to non-volatile storage. It will not be undone by a crash, power loss, or network partition.
**Database Index**
    - A special auxiliary data structure that allows your database to perform certain queries much faster.
    - Indexes can typically only exist to reference structured data, like data stored in relational databases.
    - In practice, you create an index on one or multiple columns in your database to greatly speed up **read** queries that you run very often, with the downside of slightly longer **writes** to your database, since writes have to also take place in the relevant index.
**Strong Consistency**
    - Strong Consistency usually refers to the consistency of ACID transactions, as opposed to Eventual Consistency.
**Eventual Consistency**
    - A consistency model which is unlike Strong Consistency.
    - In this model, reads might return a view of the system that is stale.
    - An eventually consistency datastore will give guarantees that the state of the database will eventually reflect writes within a time period.
**Postgres**
    - A relational database that uses a dialect of SQL called [PostgreSQL](http://postgresql.org/).
    - Provides ACID transactions.
**Key-Value Store**
    - A Key-Value Store is a flexible NoSQL database that's often used for caching and dynamic configuration.
    - Popular options include DynamoDB, Etcd, Redis, and ZooKeeper.
**Etcd**
    - [Etcd](https://etcd.io/) is a strongly consistent and highly available key-value store that's often used to implement leader election in a system.
**Redis**
    - An in-memory key-value store.
    - Does offer some persistent storage options but is typically used as a really fast, best-effort caching solution.
    - [Redis](https://redis.io/) is also often used to implement **rate limiting**.
**ZooKeeper**
    - [ZooKeeper](https://zookeeper.apache.org/) is a strongly consistent, highly available key-value store.
    - It's often used to store important configuration or to perform leader election.
**Replication**
    - The act of duplicating the data from one database server to others.
    - This is used to increase the redundancy of your system and tolerate regional failures for instance.
    - Other times, you can use replication to move data closer to your clients, thus decreasing the latency of accessing specific data.
**Sharding**
    - Sometimes called **data partitioning**, sharding is the act of splitting a database into two or more pieces called **shards** and is typically done to increase the throughput of your database.
    - Popular sharding strategies include:
      - Sharding based on a client's region.
      - Sharding based on the type of data.
      - Sharding based on the hash of a column.
**Hot Spot**
    - When distributing a workload across a set of servers, that workload might be spread unevenly.
    - This can happen if your **sharding key** or your **hashing function** are suboptimal, or if your workload is naturally skewed: some servers will receive a lot more traffic than others, thus creating a "hot spot."
**Leader Election**
    - The process by which nodes in a cluster elect a so-called "leader" amongst them, responsible for the primary operations of the service that these nodes support.
    - When correctly implemented, leader election guarantees that all nodes in the cluster know which one is the leader at any given time and can elect a new leader if the leader dies for whatever reason.
**Consensus Algorithm**
    - A type of complex algorithms used to have multiple entities agree on a single data value, like who the "leader" is amongst a group of machines.
    - Two popular consensus algorithms are **Paxos** and **Raft**.
**Paxos and Raft**
    - Two consensus algorithms that, when implemented correctly, allow for the synchronization of certain operations, even in a distributed setting.
**Peer-To-Peer Network**
    - A collection of machines referred to as peers that divide a workload between themselves to presumably complete the workload faster than would otherwise be possible.
    - Peer-to-peer networks are often used in file-distribution systems.
**Gossip Protocol**
    - When a set of machines talk to each other in a uncoordinated manner in a cluster to spread information through a system without requiring a central source of data.
**Polling**
    - The act of fetching a resource or piece of data regularly at an interval to make sure your data is not too stale.
**Streaming**
    - In networking, it usually refers to the act of continuously getting a feed of information from a server by keeping an open connection between the two machines or processes.
**Configuration**
    - A set of parameters or constants that are critical to a system.
    - Configuration is typically written in **JSON** or **YAML** and can be either:
      - **Static**, meaning that it's hard-coded and shipped with your system's application code.
      - **Dynamic**, meaning that it lives outside of your system's application code.
**Rate Limiting**
    - The act of limiting the number of requests sent to or from a system.
    - Rate limiting is most often used to limit the number of incoming requests in order to prevent **DoS attacks** and can be enforced at the IP-address level, at the user-account level, or at the region level, for example.
    - Rate limiting can also be implemented in tiers; for instance, a type of network request could be limited to 1 per second, 5 per 10 seconds, and 10 per minute.
**DoS Attack**
    - Short for "denial-of-service attack," a DoS attack is an attack in which a malicious user tries to bring down or damage a system in order to render it unavailable to users.
    - Much of the time, it consists of flooding it with traffic.
    - Some DoS attacks are easily preventable with rate limiting, while others can be far trickier to defend against.
**DDoS Attack**
    - - Short for "distributed denial-of-service attack," a DDoS attack is a DoS attack in which the traffic flooding the target system comes from many different sources, making it much harder to defend against.
**Logging**
    - The act of collecting and storing logs - useful information about events in your system.
    - Typically your programs will outut log messages to its STDOUT or STDERR pipes, which will automatically get aggregated into a **centralized logging solution**.
**Monitoring**
    - The process of having visibility into a system's key metrics, monitoring is typically implemented by collecting important events in a system and aggregating them in human-readable charts.
**Alerting**
    - The process through which system administrators get notified when critical system issues occur.
    - Alerting can be set up by defining specific thresholds on monitoring charts, past which alerts are sent to a communication channel like Slack.
**Publish/Subscribe Pattern**
    - Often shortened as **Pub/Sub**, the Publish/Subscribe pattern is a popular messaging model that consists of **publishers** and **subscribers**.
    - Publishers publish messages to special **topics** (sometimes called **channels**) without caring about or even knowing who will read those messages, and subscribers subscribe to topics and read messages coming through those topics.
    - Pub/Sub systems often come with very powerful guarantees like **at-least-once delivery**, **persistent storage**, **ordering** of messages, and **replayability** of messages.
**Idempotent Operation**
    - An operation that has the same ultimate outcome regardless of how many times it's performed.
    - If an operation can be performed multiple times without changing its overall effect, it's idempotent.
    - Operations performed through a **Pub/Sub** messaging system typically have to be idempotent, since Pub/Sub systems tend to allow the same messages to be consumed multiple times.
**[Apache Kafka](http://kafka.apache.org/)**
    - A distributed messaging system created by LinkedIn.
    - Very useful when using the **streaming** paradigm as opposed to **polling**.
**[Cloud Pub/Sub](https://cloud.google.com/pubsub/)**
    - A highly scalable Pub/Sub messaging service created by Google.
    - Guarantees **at-least-once delivery** of messages and supports "rewinding" in order to reprocess messages.
**MapReduce**
    - A popular framework for processing very large datasets in a distributed setting efficiently, quickly, and in a fault-tolerant manner. A MapReduce job is comprised of 3 main steps:
      - The **Map** step, which runs a **map function** on the various chunks of the dataset and transforms these chunks into intermediate **key-value pairs**.
      - The **Shuffle** step, which reorganizes the intermediate **key-value pairs** such that pairs of the same key are routed to the same machine in the final step.
      - The **Reduce** step, which runs a **reduce function** on the newly shuffled **key-value pairs** and transforms them into more meaningful data.
    - The canonical example of a MapReduce use case is counting the number of occurrences of words in a large text file.
    - When dealing with a MapReduce library, engineers and/or systems administrators only need to worry about map and reduce functions, as well as their inputs and outputs.
    - All other concerns, including the parallelization of tasks and the fault-tolerance of the MapReduce job, are abstracted away and taken care of by the MapReduce implementation.
**Distributed File System**
    - A Distributed File System is an abstraction over a cluster of machines that allows them to act like one large file system.
    - The two most popular implementations of a DFS are the **Google File System** (GFS) and the **Hadoop Distributed File System** (HDFS).
    - Typically, DFSs take care of the classic **availability** and **replication** guarantees that can be tricky to obtain in a distributed-system setting.
    - The overarching idea is that files are split into chunks of a certain size, and those chunks are sharded across a large cluster of machines.
    - A central control plane is in charge of deciding where each chunk resides, routing reads to the right nodes, and handling communication between machines.
    - Different DFS implementations have slightly different APIs and semantics, but they achieve the same common goal: extremely large-scale persistent storage.
**[Hadoop](https://hadoop.apache.org/)**
    - A popular, open-source framework that supports MapReduce jobs and many other kinds of data-processing pipelines.
    - Its central component is **HDFS** (Hadoop Distributed File System), on top of which other technologies have been developed.
